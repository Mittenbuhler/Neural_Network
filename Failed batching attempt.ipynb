{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "              \n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def ReLU(x):\n",
    "        return np.maximum(0.0, x)\n",
    "    \n",
    "    def __init__(self, design, step_size=0.01, activation_function=sigmoid, dropout=False, bias=False):\n",
    "        self.design = design\n",
    "        self.step_size = step_size\n",
    "        self.activation_function = activation_function # does not work properly (cannot select ReLU)\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "        self.create_weights()\n",
    "        self.activation = []\n",
    "    \n",
    "    def create_weights(self):\n",
    "        self.weights = [np.zeros(0)]\n",
    "        for i in np.arange(len(self.design)-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,[self.design[i+1], self.design[i]]))\n",
    "    \n",
    "    def one_training(self, input_data, target_data):\n",
    "        correction = self.step_size * error * self.activation[-1] * (1.0 - self.activation[-1]) @ self.activation[-2].T\n",
    "            print('correction: ' + str(correction))\n",
    "            self.weights[-1] += correction\n",
    "            \n",
    "            # Update wih weights\n",
    "            for i in np.arange(len(self.design)-2,0,-1): # move backwards through NN\n",
    "                error = self.weights[i+1].T @ error\n",
    "                correction = self.step_size * ((error * self.activation[i] * (1.0 - self.activation[i])) @ self.activation[i-1].T)\n",
    "                self.weights[i] += correction\n",
    "            \n",
    "    def train(self, input_data, target_data, n_epoches=1, batch_size=100):\n",
    "        \n",
    "        # Divide data into mini-batches\n",
    "        n_batches = int(len(input_data)/batch_size)\n",
    "        index = np.repeat(np.arange(0,n_batches,1), batch_size)\n",
    "        np.random.shuffle(index) # randomize assignment to batch    \n",
    "        overlap = len(input_data) - len(index) # fill up\n",
    "        index = np.append(index, index[:overlap])\n",
    "        \n",
    "        # Update weights for the mean error of every batch\n",
    "        for b in np.arange(n_batches):\n",
    "            print('b: ' + str(b))\n",
    "            \n",
    "            # Set up data\n",
    "            input_batch = input_data[index==b,:,:]\n",
    "            target_batch = target_data[index==b,:]\n",
    "            error = np.array([0,0], ndmin=2).T # make compatiple to different n_output_nodes!!!\n",
    "            \n",
    "            # Compute error for every data point in mini-batch\n",
    "            for d in np.arange(batch_size):\n",
    "                print('d: '+str(d))\n",
    "                # Convert input matrix and target vector to column vectors\n",
    "                input_vector = np.array(input_batch[d].flatten(), ndmin=2).T\n",
    "                target_vector = np.array(target_batch[d], ndmin=2).T\n",
    "                \n",
    "                # Compute output/activation of each layer\n",
    "                self.activation = [] # initialize activation list\n",
    "                self.activation.append(input_vector)\n",
    "                for i in np.arange(len(self.design)-1):\n",
    "                    self.activation.append(self.activation_function(self.weights[i+1] @ self.activation[i]))\n",
    "                \n",
    "                # Compute error\n",
    "                error_data_point = target_vector - self.activation[-1]\n",
    "                error = np.append(error, error_data_point, axis=1)\n",
    "                print(error_data_point)\n",
    "            \n",
    "            # Compute mean error\n",
    "            error = np.array(np.mean(error[:,1:], axis=1), ndmin=2).T\n",
    "            print('mean error: ' + str(error))\n",
    "            \n",
    "            # Update who weights\n",
    "            correction = self.step_size * error * self.activation[-1] * (1.0 - self.activation[-1]) @ self.activation[-2].T\n",
    "            print('correction: ' + str(correction))\n",
    "            self.weights[-1] += correction\n",
    "            \n",
    "            # Update wih weights\n",
    "            for i in np.arange(len(self.design)-2,0,-1): # move backwards through NN\n",
    "                error = self.weights[i+1].T @ error\n",
    "                correction = self.step_size * ((error * self.activation[i] * (1.0 - self.activation[i])) @ self.activation[i-1].T)\n",
    "                self.weights[i] += correction\n",
    "    \n",
    "    def run(self, input_data):\n",
    "        \n",
    "        # Convert data into column vector\n",
    "        input_vector = np.array(input_data.flatten(), ndmin=2).T\n",
    "        \n",
    "        # Compute layer outputs/activations\n",
    "        self.activation = [] # initialize activation list\n",
    "        self.activation.append(input_vector)\n",
    "        for i in np.arange(len(self.design)-1):\n",
    "            self.activation.append(self.activation_function(self.weights[i+1] @ self.activation[i]))\n",
    "            \n",
    "        return self.activation[-1]\n",
    "    \n",
    "    def evaluate(self, input_data, target, performance_measure=True):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
